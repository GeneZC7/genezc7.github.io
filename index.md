---
layout: default
---

## About Me

I am currently a Ph.D. student at Beijing Institute of Technology (BIT), where I am advised by [Prof. Dawei Song](http://cs.bit.edu.cn/szdw/jsml/js/sdw/index.htm). I collaborate closely with [Prof. Benyou Wang](https://wabyking.github.io/old) from CUHK-SZ on efficient language models and [Prof. Qiuchi Li](https://qiuchili.github.io) from UCPH on structural bias. I previously worked closely with [Dr. Jingang Wang](https://sites.google.com/site/bitwjg/) from Meituan NLP and [Dr. Qifan Wang](https://wqfcr.github.io/) from Meta AI on large language models. I also enjoyed building structure-grounded language models with [Binyuan Hui](https://huybery.github.io/) from Alibaba.

My current research interests lie in the general area of natural language processing, particularly efficient language models and language agents. Prior to that, I was devoted to opinion mining and model generalization.

I am on the <b><font color=red>job market 2024-2025</font></b> and actively seeking both academic and industrial positions. Drop me an email via chenzhang9702[AT]outlook[DOT]com if you are interested in collaborating with me.

## Recent Highlights

Sep. 5th, 2024. [*LongLLaVA*](https://huggingface.co/FreedomIntelligence/LongLLaVA) is released to become the first large multi-modal model that could maximally process over 1,000 images with only one Nvidia A100.

July 25th, Aug. 8th, 2024. Invited to give talks respectively on [long-context efficiency](./assets/files/Li_Auto_Long_context_Efficiency.pdf) at Li Auto and [democratization of LLMs](./assets/files/ByteDance_Research_Democratization_LLMs.pdf) at ByteDance Research.

July 9th, 2024. *MiniMA* family is now completed with released MoE model [*MiniMix*](https://huggingface.co/GeneZC/MiniMix-2_4x3B) and long-context model [*MiniLoong*](https://huggingface.co/GeneZC/MiniLoong-3B).

June 28th, 2024. One paper got accepted to TOIS.

Feb. 19th, 2024. Two long papers got accepted to [COLING 2024](https://lrec-coling-2024.org/).

Jan. 18th, 2024. One long paper got accepted to [EACL 2024](https://2024.eacl.org/).

Dec. 27th, 2023. *MiniMA* and *MiniChat* are lifted to [*MiniMA-2*](https://huggingface.co/GeneZC/MiniMA-2-3B) and [*MiniChat-2*](https://huggingface.co/GeneZC/MiniChat-2-3B) respectively. *MiniMA-2* together with *MiniMA* and other arts completes the compute-performance frontier and *MiniChat-2* surpasses Vicuna-7B on MT-Bench.

## Experiences

<ul class="timeline">
	<li>
		<div class="direction-r">
			<div class="flag-wrapper">
				<span class="flag">BIT</span>
				<span class="time-wrapper"><span class="time">Sep. 2019 - Present</span></span>
			</div>
			<div class="desc">Dual M.Eng.-Ph.D. in Computer Science</div>
		</div>
	</li>
	<li>
		<div class="direction-l">
			<div class="flag-wrapper">
				<span class="flag">Xiaohongshu NLP</span>
				<span class="time-wrapper"><span class="time">Mar. 2023 - Present</span></span>
			</div>
			<div class="desc">Algorithm Intern on Efficient Language Models</div>
		</div>
	</li>
	<li>
		<div class="direction-r">
			<div class="flag-wrapper">
				<span class="flag">CUHK-SZ</span>
				<span class="time-wrapper"><span class="time">Aug. 2022 - Mar. 2023</span></span>
			</div>
			<div class="desc">Research Assistant on Elastic Language Models</div>
		</div>
	</li>
    	<li>
		<div class="direction-l">
			<div class="flag-wrapper">
				<span class="flag">Meituan NLP</span>
				<span class="time-wrapper"><span class="time">Mar. 2021 - Mar. 2023</span></span>
			</div>
			<div class="desc">Research Intern on Large Language Models</div>
		</div>
	</li>
	<li>
		<div class="direction-l">
			<div class="flag-wrapper">
				<span class="flag">Alibaba DAMO</span>
				<span class="time-wrapper"><span class="time">Nov. 2020 - Feb. 2021</span></span>
			</div>
			<div class="desc">Research Intern on Structure-grounded Language Models</div>
		</div>
	</li>
    	<li>
		<div class="direction-l">
			<div class="flag-wrapper">
				<span class="flag">Zhejiang Lab</span>
				<span class="time-wrapper"><span class="time">July 2019 - Aug. 2019</span></span>
			</div>
			<div class="desc">Research Intern on Low-resource Information Extraction</div>
		</div>
	</li>
	<li>
		<div class="direction-r">
			<div class="flag-wrapper">
				<span class="flag">BIT</span>
				<span class="time-wrapper"><span class="time">Sep. 2015 - July 2019</span></span>
			</div>
			<div class="desc">B.Eng. in Electrical Engineering</div>
		</div>
	</li>
</ul>

## Projects

***LongLLaVA*** <br>
A hybrid-architecture large multi-modal model that becomes the first one who can process over 1,000 images with only one Nvidia A100. <br>
[[huggingface]](https://huggingface.co/FreedomIntelligence/LongLLaVA)

***MiniMA, MiniMA-2, MiniMix, MiniLoong*** <br>
A distilled language model family that establishes a new compute-performance pareto frontier among existing language models. <br>
[[github]](https://github.com/GeneZC/MiniMA)[[huggingface]](https://huggingface.co/GeneZC/MiniMA-3B)[[rank]](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)

***MiniChat, MiniChat-2*** <br>
An instruction-following language model that achieves competitive performance with a small scale. <br>
[[github]](https://github.com/GeneZC/MiniMA)[[huggingface]](https://huggingface.co/GeneZC/MiniChat-3B)[[rank]](https://tatsu-lab.github.io/alpaca_eval)

***Phoenix*** <br>
An instruction-following language model that is competitive with ChatGLM-6b. <br>
[[github]](https://github.com/FreedomIntelligence/LLMZoo)[[huggingface]](https://huggingface.co/FreedomIntelligence/phoenix-inst-chat-7b)[[rank]](https://www.superclueai.com/)[[news]](https://www.jiqizhixin.com/articles/2023-04-16-2)

***WenJin*** <br>
A large language model that reaches top-level performance on CLUE benchmark. <br>
[[rank]](https://cluebenchmarks.com/en/rank.html)

## Publications & Manuscripts [<img src="./assets/imgs/google.ico" width="22" height="22" alt="google" align=center/>](https://scholar.google.com/citations?user=IMwAXWcAAAAJ) [<img src="./assets/imgs/semantic.ico" width="22" height="22" alt="semantic" align=center/>](https://www.semanticscholar.org/author/Chen-Zhang/145107889) 
\# indicates equal contributions.

### Efficient Language Models

**MoDification: Mixture of Depths Made Easy** <br>
**Chen Zhang**, Meizhi Zhong, Qimeng Wang, Xuantao Lu, Zheyu Ye, Chengqiang Lu, Yan Gao, Yao Hu, Kehai Chen, Min Zhang, and Dawei Song. <br>
Preprint. [[arXiv]](https://arxiv.org/abs/2410.14268)

**LongLLaVA: Scaling Multi-modal LLMs to 1,000 Images Efficiently via Hybrid Architecture** <br>
Xidong Wang, Dingjie Song, Shunian Chen, **Chen Zhang** and Benyou Wang. <br>
Preprint. [[arXiv]](https://arxiv.org/abs/2409.02889)[[code]](https://github.com/FreedomIntelligence/LongLLaVA)

**Beyond the Speculative Game: A Survey of Speculative Execution in Large Language Models** <br>
**Chen Zhang**, Zhuorui Liu, Hanqing Zhang, and Dawei Song. <br>
Preprint. [[arXiv]](https://arxiv.org/abs/2404.14897)

**Towards the Law of Capacity Gap in Distilling Language Models.** <br>
**Chen Zhang**, Dawei Song, Zheyu Ye, and Yan Gao. <br>
Preprint. [[arXiv]](https://arxiv.org/abs/2311.07052)[[code]](https://github.com/GeneZC/MiniMA)

**How Speculative Can Speculative Decoding Be?** <br>
Zhuorui Liu, **Chen Zhang**, and Dawei Song. <br>
In **COLING 2024**. [[paper]](https://aclanthology.org/2024.lrec-main.725)[[poster]](./assets/files/COLING2024_HowSpec.pdf)[[code]](https://github.com/ZhuoruiLiu12/SpecGame)

**Task-agnostic Distillation of Encoder-Decoder Language Models.** <br>
**Chen Zhang**, Yang Yang, Qiuchi Li, Jingang Wang, and Dawei Song. <br>
In **COLING 2024**. [[arXiv]](https://arxiv.org/abs/2305.12330)[[poster]](./assets/files/COLING2024_MiniEnD.pdf)[[code]](https://github.com/GeneZC/MiniEnD)

**Lifting the Curse of Capacity Gap in Distilling Language Models.** <br>
**Chen Zhang**, Yang Yang, Jiahao Liu, Jingang Wang, Yunsen Xian, Benyou Wang, and Dawei Song.  <br>
In **ACL 2023**. [[arXiv]](https://arxiv.org/abs/2305.12129)[[slides]](./assets/files/ACL2023_MiniMoE.pdf)[[code]](https://github.com/GeneZC/MiniMoE)

**On Elastic Language Models.** <br>
**Chen Zhang**, Benyou Wang, and Dawei Song. <br>
In **TOIS**. [[arXiv]](https://arxiv.org/abs/2311.07204)

**Minimal Distillation Schedule for Extreme Language Model Compression.** <br>
**Chen Zhang**, Yang Yang, Qifan Wang, Jiahao Liu, Jingang Wang, Wei Wu, and Dawei Song. <br>
In **EACL 2024** Findings. [[arXiv]](https://arxiv.org/abs/2205.14570)[[poster]](./assets/files/EACL2024_MiniDisc.pdf)[[code]](https://github.com/GeneZC/MiniDisc) <br>

**Sparse Teachers Can Be Dense with Knowledge.** <br>
Yi Yang\#, **Chen Zhang**\#, and Dawei Song.  <br>
In **EMNLP 2022**. [[arXiv]](https://arxiv.org/abs/2210.03923)[[poster]](./assets/files/EMNLP2022_StarK.pdf)[[code]](https://github.com/GeneZC/StarK)

### Language Agents

**Phoenix: Democratizing ChatGPT across Languages.** <br>
Zhihong Chen, Feng Jiang, Junying Chen, Tiannan Wang, Fei Yu, Guiming Chen, Hongbo Zhang, Juhao Liang, **Chen Zhang**, Zhiyi Zhang, Jianquan Li, Xiang Wan, Benyou Wang, and Haizhou Li. <br>
Preprint. [[arXiv]](https://arxiv.org/abs/2304.10453)[[code]](https://github.com/FreedomIntelligence/LLMZoo)

### Opinion Mining

**PyABSA: A Modularized Framework for Reproducible Aspect-based Sentiment Analysis.** <br>
Heng Yang, **Chen Zhang**, and Ke Li. <br>
In **CIKM 2023** Demo. [[arXiv]](https://arxiv.org/abs/2208.01368)[[code]](https://github.com/yangheng95/PyABSA) 

**Structural Bias For Aspect Sentiment Triplet Extraction.** <br>
**Chen Zhang**, Lei Ren, Fang Ma, Jingang Wang, Wei Wu, and Dawei Song. <br>
In **COLING 2022**. [[arXiv]](https://arxiv.org/abs/2209.00820)[[slides]](./assets/files/COLING2022_StructBias.pdf)[[code]](https://github.com/GeneZC/StructBias)[[data]](https://github.com/GeneZC/StructBias)[[blog]](https://tech.meituan.com/2021/10/20/the-applications-of-sentiment-analysis-meituan.html)

**Aspect-specific Context Modeling for Aspect-based Sentiment Analysis.** <br>
Fang Ma, **Chen Zhang**, Bo Zhang, and Dawei Song. <br>
In **NLPCC 2022**. [[arXiv]](https://arxiv.org/abs/2207.08099)[[slides]](./assets/files/NLPCC2022_AspCon.pdf)[[data]](https://github.com/GeneZC/advABSA)

**Exploiting Position Bias for Robust Aspect Sentiment Classification.** <br>
Fang Ma\#, **Chen Zhang**\#, and Dawei Song. <br>
In **ACL 2021** Findings. [[arXiv]](https://arxiv.org/abs/2105.14210)[[slides]](./assets/files/ACL2021_PosBias.pdf)[[code]](https://github.com/BD-MF/POS4ASC)

**End-to-end Emotion-Cause Pair Extraction via Learning to Link.** <br>
Haolin Song, **Chen Zhang**, Qiuchi Li, and Dawei Song. <br>
Preprint. [[arXiv]](https://arxiv.org/abs/2002.10710)[[code]](https://github.com/shl5133/E2EECPE)

**A Multi-task Learning Framework for Opinion Triplet Extraction.** <br>
**Chen Zhang**, Qiuchi Li, Dawei Song, and Benyou Wang. <br>
In **EMNLP 2020** Findings. [[arXiv]](https://arxiv.org/abs/2010.01512)[[paper]](https://www.aclweb.org/anthology/2020.findings-emnlp.72/)[[code]](https://github.com/GeneZC/OTE-MTL)

**Aspect-based Sentiment Classification with Aspect-specific Graph Convolutional Networks.** <br>
**Chen Zhang**, Qiuchi Li, and Dawei Song. <br>
In **EMNLP 2019**. [[arXiv]](https://arxiv.org/abs/1909.03477)[[slides]](./assets/files/EMNLP2019_ASGCN.pdf)[[code]](https://github.com/GeneZC/ASGCN)

**Syntax-Aware Aspect-Level Sentiment Classification with Proximity-Weighted Convolution Network.** <br>
**Chen Zhang**, Qiuchi Li, and Dawei Song. <br>
In **SIGIR 2019**. [[arXiv]](https://arxiv.org/abs/1909.10171)[[poster]](./assets/files/SIGIR2019_PWCN.pdf)[[code]](https://github.com/GeneZC/PWCN)

### Model Generalization

**Modular Retrieval for Generalization and Interpretation.** <br>
Juhao Liang, **Chen Zhang**, Zhengyang Tang, Jie Fu, Dawei Song, and Benyou Wang. <br>
Preprint. [[arXiv]](https://arxiv.org/abs/2303.13419)[[code]](https://github.com/FreedomIntelligence/REMOP)

**XPrompt: Exploring the Extreme of Prompt Tuning.** <br>
Fang Ma, **Chen Zhang**, Lei Ren, Jingang Wang, Qifan Wang, Wei Wu, Xiaojun Quan, and Dawei Song. <br>
In **EMNLP 2022**. [[arXiv]](https://arxiv.org/abs/2210.04457)[[poster]](./assets/files/EMNLP2022_XPrompt.pdf)

**Making Pretrained Language Models Good Long-tailed Learners.** <br>
**Chen Zhang**, Lei Ren, Jingang Wang, Wei Wu, and Dawei Song. <br>
In **EMNLP 2022**. [[arXiv]](https://arxiv.org/abs/2205.05461)[[poster]](./assets/files/EMNLP2022_Glee.pdf)[[code]](https://github.com/GeneZC/Glee)

**<img src="./assets/imgs/dogetickets.png" width="25" height="15" alt="dogetickets" align=center/> Doge Tickets: Uncovering Domain-general Language Models by Playing Lottery Tickets.** <br>
Yi Yang\#, **Chen Zhang**\#, Benyou Wang, and Dawei Song. <br> 
In **NLPCC 2022, Best Paper Award**. [[arXiv]](https://arxiv.org/abs/2207.09638)[[slides]](./assets/files/NLPCC2022_DogeTickets.pdf)[[code]](https://github.com/Ylily1015/DogeTickets)

**Adaptable Text Matching via Meta-Weight Regulator.** <br>
Bo Zhang, **Chen Zhang**, Fang Ma, and Dawei Song. <br>
In **SIGIR 2022**. [[arXiv]](https://arxiv.org/abs/2204.12668)[[paper]](https://dl.acm.org/doi/10.1145/3477495.3531932)[[slides]](./assets/files/SIGIR2022_MWR.pdf)

**A Simple Baseline for Cross-domain Few-shot Text Classification.** <br>
**Chen Zhang** and Dawei Song. <br>
In **NLPCC 2021**. [[paper]](https://link.springer.com/chapter/10.1007/978-3-030-88480-2_56)[[slides]](./assets/files/NLPCC2021_XFew.pdf)[[code]](https://github.com/GeneZC/XFew)

## Talks

**On Long-context Efficiency** at Li Auto. 2024/7/25. [[slides]](./assets/files/Li_Auto_Long_context_Efficiency.pdf)

**Democratization of LLMs** at ByteDance Research. 2024/8/8. [[slides]](./assets/files/ByteDance_Research_Democratization_LLMs.pdf)

## Services

Organizer: WSDM CUP 2024.

Reviewer: ARR, ACL, EMNLP, NAACL, SIGIR, CIKM, AAAI.

Secondary Reviewer: WSDM, ICTIR, TOIS.

Volunteer: EMNLP.

## Honors & Awards

**Best Paper Award** at NLPCC. 2022.

**Elite Ph.D. Student** at BIT. 2021.

**XIAOMI Scholarship**. 2021.

**Excellent Undergraduate & Graduation Thesis** at BIT. 2019.

**SIGIR Student Travel Grant** by SIGIR. 2019.

**Excellent Prize** in International Collegiate Competition for Brain-inspired Computing. 2018.

**Quite A Few Medals** from Chinese (CCGC) and International (TAAI, ICGA) Computer Games Competition. 2017, 2018, 2019.

**Third Prize** at China University ROBOt COmpetitioN (ROBOCON), member of Robot Team DreamChaser at BIT. 2018.

**First Prize** at China Undergraduate Mathematical Contest in Modeling, Beijing Division. 2016.
